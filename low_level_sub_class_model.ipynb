{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "low_level_sub_class_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z4Z9PKJfSBdv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class my_layer(tf.keras.layers.Layer) : \n",
        "  def __init__(self , units ,activation =None): \n",
        "    super(my_layer , self).__init__()\n",
        "    self.units = units\n",
        "    self.activation = tf.keras.activations.get(activation)\n",
        "  def build(self , input_shape) : \n",
        "    W_i = tf.random_normal_initializer( )\n",
        "    self.w = tf.Variable(W_i(shape=(input_shape[-1] , self.units)) , dtype='float32' , trainable=True)\n",
        "    b_i = tf.zeros_initializer()\n",
        "    self.b = tf.Variable(b_i(shape=(self.units ,) ),dtype='float32' , trainable=True )\n",
        "\n",
        "  def call(self , input) : \n",
        "    return self.activation(tf.matmul(input , self.w) + self.b)"
      ],
      "metadata": {
        "id": "u_CNpTfXSL4O"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class my_model(tf.keras.Model) : \n",
        "  def __init__(self, n_classes) :\n",
        "    super(my_model , self).__init__()\n",
        "    self.dense1 = my_layer(265 , 'relu')\n",
        "    self.dense2 = my_layer(64 , 'relu')\n",
        "    self.out = my_layer(n_classes, 'softmax')\n",
        "  def call(self,input) : \n",
        "    X = self.dense1(input)\n",
        "    X = self.dense2(X)\n",
        "    return self.out(X) "
      ],
      "metadata": {
        "id": "mNTdb7MDUILB"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Qp-hMBUX-2",
        "outputId": "eba15e19-1b28-4bc7-def4-97babba316cb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.my_model at 0x7faf5ce5c110>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_metrix = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "val_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "val_metrix  =tf.keras.metrics.SparseCategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "SQXUVMuqUZyS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_step_gradient(model , optimizer , x ,y ,train_loss , train_metrix):\n",
        "  with tf.GradientTape() as tape : \n",
        "    preds =model(x) \n",
        "    loss = train_loss( y ,preds )\n",
        "  gradients = tape.gradient(loss , model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients , model.trainable_weights))\n",
        "  return loss , preds  "
      ],
      "metadata": {
        "id": "nnH8ui12Vg1g"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_process(model ,optimizer , x , y , train_loss , train_metrix , verbose = True, num_epochs =20) : \n",
        "  train_losses = []\n",
        "  val_loss = []\n",
        "  for epoch in range(num_epochs) : \n",
        "    loss , logits  = apply_step_gradient(model , optimizer , x , y ,train_loss , train_metrix) \n",
        "    train_losses.append(loss)\n",
        "    logits = tf.round(logits)\n",
        "    logits = tf.cast(logits, 'int64')\n",
        "    train_metrix.update_state( y,logits  )\n",
        "    if verbose:\n",
        "            print(\"Training loss for step %s: %.4f\" % (int(epoch), float(loss)))\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "2J48zHRlW2pc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train , T_labels) , (test , labels ) = tf.keras.datasets.mnist.load_data()\n",
        "# train /= 255 "
      ],
      "metadata": {
        "id": "iYIOZUdVZDoV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train /255\n"
      ],
      "metadata": {
        "id": "SHLephsKZcJL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = tf.reshape(train , ( -1 ,28*28))\n"
      ],
      "metadata": {
        "id": "vF7ijE6KaJBq"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = tf.cast(train , dtype='float32')"
      ],
      "metadata": {
        "id": "eK6ZuFvsbzqu"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = my_model(10) \n",
        "losses = train_process(model , optimizer , train , T_labels , train_loss , train_metrix )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz4vQ1fuZhDC",
        "outputId": "34c34595-9fd1-4bfa-9e63-83e420c596a5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss for step 0: 2.2991\n",
            "Training loss for step 1: 2.2785\n",
            "Training loss for step 2: 2.2515\n",
            "Training loss for step 3: 2.2201\n",
            "Training loss for step 4: 2.1838\n",
            "Training loss for step 5: 2.1410\n",
            "Training loss for step 6: 2.0912\n",
            "Training loss for step 7: 2.0345\n",
            "Training loss for step 8: 1.9713\n",
            "Training loss for step 9: 1.9019\n",
            "Training loss for step 10: 1.8265\n",
            "Training loss for step 11: 1.7452\n",
            "Training loss for step 12: 1.6584\n",
            "Training loss for step 13: 1.5678\n",
            "Training loss for step 14: 1.4755\n",
            "Training loss for step 15: 1.3829\n",
            "Training loss for step 16: 1.2903\n",
            "Training loss for step 17: 1.1988\n",
            "Training loss for step 18: 1.1102\n",
            "Training loss for step 19: 1.0263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iqr4wopuZ2fI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}